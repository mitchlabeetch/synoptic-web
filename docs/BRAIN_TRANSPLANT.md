# ğŸ§  Brain Transplant: Vector RAG Architecture

## Overview

This document describes the migration from DigitalOcean's managed GenAI Knowledge Base to a self-hosted RAG (Retrieve-And-Generate) system using pgvector.

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        RAG PIPELINE                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  1. USER QUERY          2. LOCAL EMBEDDING        3. VECTOR DB   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ "Explain    â”‚ â”€â”€â”€â–¶  â”‚ all-MiniLM-L6   â”‚ â”€â”€â”€â–¶ â”‚ PostgreSQL  â”‚ â”‚
â”‚  â”‚  the French â”‚       â”‚ (CPU, 384 dims) â”‚      â”‚ + pgvector  â”‚ â”‚
â”‚  â”‚  subjunc-   â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚             â”‚ â”‚
â”‚  â”‚  tive..."   â”‚                                â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚ â”‚knowledgeâ”‚ â”‚ â”‚
â”‚                                                 â”‚ â”‚_base    â”‚ â”‚ â”‚
â”‚  4. RETRIEVED           5. AUGMENTED PROMPT     â”‚ â”‚(HNSW)   â”‚ â”‚ â”‚
â”‚     CONTEXT                                     â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”‚ Top 3 most  â”‚ â”€â”€â”€â–¶  â”‚ SYSTEM: You are â”‚                      â”‚
â”‚  â”‚ relevant    â”‚       â”‚ The Philologist. â”‚                     â”‚
â”‚  â”‚ grammar     â”‚       â”‚ REFERENCE:       â”‚                     â”‚
â”‚  â”‚ chunks      â”‚       â”‚ [retrieved ctx]  â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚ USER: [query]    â”‚                     â”‚
â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                               â”‚                                  â”‚
â”‚                               â–¼                                  â”‚
â”‚  6. LLM INFERENCE      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚                        â”‚ Llama 3.3 70B   â”‚                      â”‚
â”‚                        â”‚ (DO Gradient)   â”‚                      â”‚
â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                               â”‚                                  â”‚
â”‚                               â–¼                                  â”‚
â”‚  7. RESPONSE           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚                        â”‚ Structured JSON â”‚                      â”‚
â”‚                        â”‚ with explanationâ”‚                      â”‚
â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Cost Comparison

| Component          | Managed GenAI | Manual RAG          |
| ------------------ | ------------- | ------------------- |
| Knowledge Base     | ~â‚¬15-25/mo    | â‚¬0 (in existing PG) |
| OpenSearch Cluster | ~â‚¬20-40/mo    | â‚¬0 (pgvector)       |
| Embeddings         | API cost      | â‚¬0 (local CPU)      |
| LLM Inference      | Same          | Same                |
| **Total Savings**  |               | **~â‚¬35-65/mo**      |

## Files Created

```
synoptic-web/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ nuke-kb.js              # ğŸ—‘ï¸  Delete expensive DO resources
â”‚   â”œâ”€â”€ run-vector-migration.js # ğŸ§¬  Apply vector schema
â”‚   â””â”€â”€ seed-knowledge.ts       # ğŸ§   Ingest grammar files
â”œâ”€â”€ migrations/
â”‚   â””â”€â”€ 001_add_vectors.sql     # ğŸ“Š  Vector schema + HNSW index
â””â”€â”€ src/lib/ai/providers/
    â””â”€â”€ rag.ts                  # ğŸ”Œ  RAG provider implementation
```

## Deployment Steps

### Step 1: Nuke Old Infrastructure (Optional)

```bash
# Delete expensive DO resources (GenAI agents, KB, OpenSearch)
DO_API_TOKEN=your_token node scripts/nuke-kb.js
```

### Step 2: Apply Database Migration

**Option A: From DigitalOcean Console**

1. Go to Databases â†’ Your Cluster â†’ Query Editor
2. Paste contents of `migrations/001_add_vectors.sql`
3. Execute

**Option B: From Allowed IP/VPN**

```bash
DB_PASSWORD=your_password node scripts/run-vector-migration.js
```

**Option C: Via App Platform Build Phase**
Add to your `package.json`:

```json
{
  "scripts": {
    "postinstall": "node scripts/run-vector-migration.js"
  }
}
```

### Step 3: Seed Knowledge Base

```bash
# First time will download ~90MB model (cached after)
DB_PASSWORD=your_password npx ts-node scripts/seed-knowledge.ts
```

This ingests all 33 translation guide markdown files (~300KB total):

- Chunks by H2 headers (##)
- Generates 384-dim embeddings locally
- Stores in PostgreSQL with HNSW index

### Step 4: Update Environment Variables

```env
# Remove old GenAI agent IDs (optional)
# NEXT_PUBLIC_AI_AGENT_LINGUIST_ID=...
# NEXT_PUBLIC_AI_AGENT_PHILOLOGIST_ID=...

# Add direct inference keys
DO_LINGUIST_KEY=XvuNZ86iTN85pIX-VPF0vBWWansLee9H
DO_PHILOLOGIST_KEY=d5wc5BJb0sVc81phFQIvFWiAB0Zn0Y1m

# Set provider to RAG (or leave unset - it's the default now)
AI_PROVIDER=rag
```

### Step 5: Deploy

```bash
git add .
git commit -m "ğŸ§  Brain Transplant: Migrate to self-hosted RAG"
git push
```

## Verification

### Check Knowledge Base Stats

```sql
SELECT
  language,
  COUNT(*) as chunks,
  AVG(LENGTH(content)) as avg_length
FROM knowledge_base
GROUP BY language
ORDER BY chunks DESC;
```

### Test Semantic Search

```sql
-- This requires running the embedding through the model first
-- In practice, the RAG provider does this automatically
SELECT
  section_title,
  LEFT(content, 100) as preview
FROM knowledge_base
WHERE language = 'fr'
LIMIT 5;
```

## Troubleshooting

### "extension 'vector' does not exist"

pgvector isn't installed. On DigitalOcean Managed PostgreSQL:

1. Go to Database Settings
2. Enable the `vector` extension
3. Re-run migration

### "connect ETIMEDOUT"

Your IP isn't allowed. Options:

1. Add your IP to "Trusted Sources" in DO database settings
2. Run migration from App Platform (has automatic access)
3. Use VPN connected to DO network

### Embedding model download stuck

The first run downloads ~90MB model. If stuck:

```bash
# Clear cache and retry
rm -rf ~/.cache/huggingface
npx ts-node scripts/seed-knowledge.ts
```

## Provider Comparison

| Provider       | Use Case                          | Cost          |
| -------------- | --------------------------------- | ------------- |
| `rag`          | **Default** - Best for production | Lowest        |
| `gradient`     | Legacy managed agents             | Higher        |
| `openai`       | Backup/testing                    | Pay-per-token |
| `digitalocean` | Deprecated                        | N/A           |

## Future Improvements

1. **Streaming**: Add streaming support for real-time responses
2. **Caching**: Cache frequent queries to reduce embedding overhead
3. **Reranking**: Add cross-encoder reranking for better relevance
4. **Multi-modal**: Support image embeddings for visual content
